---
title: "p8105_hw5_mk4992"
author: "Maryam Khalid"
date: "2025-11-15"
output: html_document
---

```{r}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
library(tidyverse)
library(broom)
library(janitor)
library(purrr)

set.seed(8105)

```

*Problem 1: Probability of Shared Birthdays*
We investigate the probability that at least two people in a room of *n* people share a birthday. 
We assume 365 equally likely birthdays and no leap years. 
```{r}
simulate_shared_birthday <- function(n) {
birthdays <- sample(1:365, size = n, replace = TRUE)
any(duplicated(birthdays))
}

#to run a simulation with group size of n to get T/F, do
#simuluate_shared_birthday(n)

#to get probability from z runs:
#mean(replicate(z, simulate_shared_birthay(n)))

#vector of various group sizes
group_sizes <- 2:50

#run 1000 simulations per group size
sim_results <-
  tibble(group_size = group_sizes) %>%
  mutate(
    shared_prob = map_dbl(
      group_sizes,
      ~mean(replicate(1000, simulate_shared_birthday(.x)))
    )
  )

sim_results %>%
  ggplot(aes(x = group_size, y = shared_prob)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  labs(
    title = "Probability of At Least One Shared Birthday",
    x = "Group Size",
    y = "Estimated Probability"
  ) +
  theme_minimal(base_size = 14)

```

The curve representing the probability of at least one shared birthday looks similar to a logistic/sigmoidal growth curve. As the group size increases, the estimated probability increases following this growth curve. This probability is near zero for single-digit sizes, rises quickly between ~15 to 30 people, amd then levels off approaching 1. The probability is 0.5 when the group size is 23 people. By n = 50, the probability is ~0.973. Shared birthdays become extremely likely way before the group size reaches 365 people. 

*Problem 2: Hypothesis Testing from a Normal Distribution*
The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. We conduct a simulation to explore power in a one-sample t-test.

```{r}
set.seed(8105)

#fixed parameters of our normal distribution
n <- 30
sigma <- 5
mu_values <- 0:6 #includes 0 through 6

#function will simulate dataset and return the estimated mean (mu_hat) and p-value
simulate_power <- function(mu) {
  x <- rnorm(n, mean = mu, sd = sigma)
  test <- t.test(x, mu = 0)
  
  tibble(
    mu_hat = mean(x),
    p_value = tidy(test)$p.value
  )
}

#repeat 5000 simulations for each value of mu
sim_results <- 
  crossing(mu_true = mu_values, sim = 1:5000) %>%
  mutate(result = map(mu_true, simulate_power)) %>%
  unnest(result)

#Plot the Power versus True Mean
power_results <-
  sim_results %>%
  group_by(mu_true) %>%
  summarize(power = mean(p_value < 0.05))

power_results %>%
  ggplot(aes(x = mu_true, y = power)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  labs(
    title = "Power of One-Sample t-test vs Effect Size",
    x = "True Mean (μ)",
    y = "Power (Probability of Rejecting Null H)"
  ) +
  theme_minimal(base_size = 14)


#Plot the Mean Estimate versus True Mean
mu_hat_summary <-
  sim_results %>%
  group_by(mu_true) %>%
  summarize(
    mean_mu_hat = mean(mu_hat),
    mean_mu_hat_rejected = mean(mu_hat[p_value < 0.05])
  )

mu_hat_summary %>%
  ggplot(aes(x = mu_true)) +
  geom_line(aes(y = mean_mu_hat, color = "All samples"), linewidth = 1) +
  geom_line(aes(y = mean_mu_hat_rejected, color = "Rejected only"), linewidth = 1) +
  scale_color_manual(values = c("All samples" = "black", "Rejected only" = "red")) +
  labs(
    title = "Average Estimates of μ̂ vs True μ",
    x = "True Mean (μ)",
    y = "Average Estimate of μ̂",
    color = ""
  ) +
  theme_minimal(base_size = 14)


```

Power increases as the true mean (μ) increases. When μ is close to 0, the test rarely rejects the null hypothesis, but the power rises sharply as μ becomes larger. When μ = 6, the t-test has nearly perfect power. This reflects the relationship between effect size and statistical power.

Across all samples, the average estimate of μ^ closely matches the true mean μ, as the indicated line on the graph closely follows y = x. This demonstrates that the estimator is unbiased. However, when restricting to only the samples in which the null hypothesis was rejected, the average μ^ is somewhat inflated above the true values of μ. This occurs because only samples with large observed effects (leading to rejected null) tend to be statistically significant, introducing selection bias. 

*Problem 3: Homicide Data Across U.S. Cities*
The Washington Post has gathered data on over 52,000 homicides in 50 large U.S. cities over the past decade and found that across the country, there are areas where murder is common but arrests are rare.

```{r}
homicide_raw <- read_csv("data/homicide-data.csv")

homicide_raw %>%
  glimpse()

```

The raw dataset includes 52,179 homicide records from 50 large U.S. cities between the years 2007 and 2017. Each observation represents a single homicide case. The dataset contains variables describing the victim (first and last name, race, age, sex), date in which the case was first reported, location (city, state, longitude, latitude), and the status of the case (case closed by arrest, open/no arrest, case closed without arrest). 

```{r}
#Create city_state variable and summarize within cities to obtain total # of homicides & # of unsolved homicides

homicide_city_summary <- 
  homicide_raw %>%
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarize(
    total = n(),
    unsolved = sum(unsolved),
    percentage_unsolved = unsolved/total * 100,
    .groups = "drop"
  )

```


```{r}
#estimate the proportion of unsolved homicides in Baltimore, MD
baltimore <- 
  homicide_raw %>%
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  filter(city_state == "Baltimore, MD")

# Run prop.test
baltimore_test <- prop.test(
  x = sum(baltimore$unsolved),
  n = nrow(baltimore)
)

# Tidy results
baltimore_summary <- 
  broom::tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high)

baltimore_summary


```

The estimated proportion of unsolved homicides in Baltimore, MD is 64.6%, with the 95% CI allowing this estimated proportion to range between 62.8% to 66.3%.

```{r}
city_prop_tests <- 
  homicide_raw %>%
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarize(
    x = sum(unsolved),
    n = n(),
    .groups = "drop"
  ) %>%
  mutate(
    test = map2(x, n, ~ prop.test(.x, .y)),
    tidy = map(test, broom::tidy)
  ) %>%
  unnest(tidy) %>%
  select(city_state, estimate, conf.low, conf.high)

```

```{r}
city_prop_tests %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "blue", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
                width = 0.2,
                color = "blue") +
  coord_flip() +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion (with 95% CI)"
  ) +
  theme_minimal(base_size = 14)
```

The estimated proportion of unsolved homicides  -- homicides that remain open or were closed without an arrest -- varies substantially across the cities, ranging from as low as 25% to as high as 75%.

Some cities such as Richmond, VA and Charlotte, NC show relatively low unsolved rates. Other cities such as Chicago, IL, New Orleans, LA, and Baltimore, MD show much higher proportions of unsolved cases. 

